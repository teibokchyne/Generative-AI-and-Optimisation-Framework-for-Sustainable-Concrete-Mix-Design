{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Imports](#imports)\n",
    "- [Load Data](#load-data)\n",
    "- [Split Data](#split-data)\n",
    "- [Normalizing Data](#normalizing-data)\n",
    "- [Conditional Variational Autoencoder Definition](#conditional-variational-autoencoder-definition)\n",
    "- [Training CVAE](#training-cvae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import csv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_filepath = r\"Wiley.csv\"\n",
    "\n",
    "wiley_data = pd.read_csv(data_filepath)\n",
    "# remove\n",
    "df = wiley_data.drop(['Index','Unnamed: 16'], axis=1)\n",
    "\n",
    "  # Update with your actual CSV file path\n",
    "\n",
    "# Separate input features (X) and output feature (Y)\n",
    "X = df.drop(columns=['fc (MPa)']).values  # All features except target\n",
    "Y = df[['fc (MPa)']].values  # Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data\n",
    "Make sure that data is split between train and test so that MinMaxScaler will not have problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid split found on attempt 3\n",
      "\n",
      "Train set min-max (X):\n",
      "       Min     Max\n",
      "0   270.0  1251.2\n",
      "1     0.0   433.7\n",
      "2     0.0   375.0\n",
      "3     0.0   356.0\n",
      "4     0.0   397.0\n",
      "5     0.0   772.2\n",
      "6     0.0    38.0\n",
      "7     0.0   234.0\n",
      "8     0.0  1502.8\n",
      "9     0.0  1195.0\n",
      "10   90.0   234.0\n",
      "11    1.1    57.0\n",
      "12   20.0   200.0\n",
      "13   50.0   100.0\n",
      "14    1.0   365.0\n",
      "\n",
      "Test set min-max (X):\n",
      "        Min     Max\n",
      "0   401.00  1251.2\n",
      "1     0.00   433.7\n",
      "2     0.00   360.0\n",
      "3     0.00   270.0\n",
      "4     0.00   397.0\n",
      "5     0.00   772.2\n",
      "6     0.00    34.5\n",
      "7     0.00   234.0\n",
      "8     0.00  1502.8\n",
      "9     0.00  1138.0\n",
      "10  126.00   234.0\n",
      "11    3.85    57.0\n",
      "12   20.00    90.0\n",
      "13   60.00   100.0\n",
      "14    1.00   365.0\n",
      "\n",
      "Train set min-max (Y):\n",
      "     Min    Max\n",
      "0  32.5  220.5\n",
      "\n",
      "Test set min-max (Y):\n",
      "     Min    Max\n",
      "0  39.1  206.6\n"
     ]
    }
   ],
   "source": [
    "# The min and max must be checked to ensure that scaling is performed properly.\n",
    "# The min and max of each feature in the test must be within the range of min and max of the \n",
    "# corresponding feature in the train.\n",
    "# Otherwise, the scaling of the test will generate greater than 1 or less than 0 values.\n",
    "# Thus, we will find the min and max and make sure they follow the above conditions.\n",
    "\n",
    "# Define function to check min-max conditions for both X and Y\n",
    "def min_max_check(train, test):\n",
    "    min_train, max_train = train.min(axis=0), train.max(axis=0)\n",
    "    min_test, max_test = test.min(axis=0), test.max(axis=0)\n",
    "    \n",
    "    return np.all(min_train <= min_test) and np.all(max_train >= max_test)\n",
    "\n",
    "# Try splitting up to 10 times\n",
    "for attempt in range(10):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42 + attempt)\n",
    "\n",
    "    if min_max_check(X_train, X_test) and min_max_check(Y_train, Y_test):\n",
    "        print(f\"Valid split found on attempt {attempt + 1}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Failed to find a valid split after 10 attempts.\")\n",
    "\n",
    "# Report min and max values\n",
    "train_min_max_X = pd.DataFrame({'Min': X_train.min(axis=0), 'Max': X_train.max(axis=0)})\n",
    "test_min_max_X = pd.DataFrame({'Min': X_test.min(axis=0), 'Max': X_test.max(axis=0)})\n",
    "\n",
    "train_min_max_Y = pd.DataFrame({'Min': Y_train.min(axis=0), 'Max': Y_train.max(axis=0)})\n",
    "test_min_max_Y = pd.DataFrame({'Min': Y_test.min(axis=0), 'Max': Y_test.max(axis=0)})\n",
    "\n",
    "print(\"\\nTrain set min-max (X):\\n\", train_min_max_X)\n",
    "print(\"\\nTest set min-max (X):\\n\", test_min_max_X)\n",
    "\n",
    "print(\"\\nTrain set min-max (Y):\\n\", train_min_max_Y)\n",
    "print(\"\\nTest set min-max (Y):\\n\", test_min_max_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scalers\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_Y = MinMaxScaler()\n",
    "\n",
    "# Fit on training data only and transform both sets\n",
    "X_train_normalized = scaler_X.fit_transform(X_train)\n",
    "X_test_normalized = scaler_X.transform(X_test)  # Use transform, not fit_transform\n",
    "\n",
    "Y_train_normalized = scaler_Y.fit_transform(Y_train)\n",
    "Y_test_normalized = scaler_Y.transform(Y_test)  # Use transform, not fit_transform\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_normalized, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test_normalized, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "# batch_size = 64 original\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for cost and embodied carbon calculation\n",
    "\n",
    "## Material Costs and Embodied Carbon  \n",
    "\n",
    "### Based on the average prices per kg as of 4.3.2025 from [IndiaMART](https://dir.indiamart.com)  \n",
    "\n",
    "| Material             | Price (₹/kg) |\n",
    "|----------------------|-------------|\n",
    "| Cement              | 6.25        |\n",
    "| Silica fume         | 25          |\n",
    "| Blast furnace slag  | 10          |\n",
    "| Fly ash             | 1           |\n",
    "| Quarry powder       | 21          |\n",
    "| Limestone powder    | 3           |\n",
    "| Nano Silica         | 965         |\n",
    "| Fiber               | 75          |\n",
    "| Sand                | 2.3         |\n",
    "| Gravel              | 2           |\n",
    "| Superplasticizer    | 61          |\n",
    "\n",
    "### Embodied Carbon (ICE Advanced Database)  \n",
    "\n",
    "*Source: [ICE Advanced Database](https://circularecology.com/ice-database-faqs-2.html?utm_source=chatgpt.com)*  \n",
    "**(Make sure to follow citation rules for ICE Database)**  \n",
    "\n",
    "| Material             | Embodied Carbon (kg CO₂/kg) |\n",
    "|----------------------|---------------------------|\n",
    "| Cement              | 0.84                        |\n",
    "| Blast furnace slag  | 0.08                        |\n",
    "| Limestone powder    | 0.02                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost and Embodied Carbon Data\n",
    "material_costs = np.array([6.25, 25, 10, 1, 21, 3, 965, 75, 2.3, 2, 0, 61, 0, 0, 0]) # Cost per kg\n",
    "embedded_carbon = np.array([0.84, 0, 0.08, 0, 0, 0.02, 0, 0, 0, 0, 0, 0, 0, 0, 0])  # Embodied carbon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Variational Autoencoder definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source for the formulae is official PyTorch tutorial\n",
    "# Define CVAE model\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, cond_dim, latent_dim, encoder_layers, decoder_layers ):\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        # Encoder: Learns (z | X, Y)\n",
    "        encoder_sequential_layers = []\n",
    "        in_dim = input_dim+cond_dim\n",
    "        for out_dim in encoder_layers:\n",
    "            encoder_sequential_layers.append(nn.Linear(in_dim,out_dim))\n",
    "            encoder_sequential_layers.append(nn.ReLU())\n",
    "            in_dim = out_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_sequential_layers)\n",
    "\n",
    "        self.mu = nn.Linear(encoder_layers[-1], latent_dim)\n",
    "        self.logvar = nn.Linear(encoder_layers[-1], latent_dim)\n",
    "\n",
    "        # Decoder: Learns (X' | z, Y)\n",
    "        decoder_sequential_layers = []\n",
    "        in_dim = latent_dim + cond_dim\n",
    "        for out_dim in decoder_layers:\n",
    "            decoder_sequential_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            decoder_sequential_layers.append(nn.ReLU())\n",
    "            in_dim = out_dim\n",
    "\n",
    "        # Final layer maps to original input dimension\n",
    "        decoder_sequential_layers.append(nn.Linear(in_dim, input_dim))\n",
    "        decoder_sequential_layers.append(nn.Sigmoid())  # If output is scaled between 0 and 1\n",
    "\n",
    "        self.decoder = nn.Sequential(*decoder_sequential_layers)\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        inputs = torch.cat((x, y), dim=1)  # Concatenate input features with condition\n",
    "        h = self.encoder(inputs)\n",
    "        return self.mu(h), self.logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std  # Reparametrization trick\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        inputs = torch.cat((z, y), dim=1)  # Concatenate latent space with condition\n",
    "        return self.decoder(inputs)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mu, logvar = self.encode(x, y)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z, y), mu, logvar\n",
    "\n",
    "# Define loss function: Reconstruction Loss (MSE) + KL Divergence\n",
    "# Define loss function: Reconstruction Loss (MSE) + KL Divergence\n",
    "def loss_function(recon_x, x, mu, logvar,c1=0.000001,c2=0.001):\n",
    "    # MSE Loss (Reconstruction Loss)\n",
    "    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum') \n",
    "    \n",
    "    # KLD (KL Divergence)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Denormalizing the inputs and reconstructions\n",
    "    X_denormalized = torch.tensor(scaler_X.inverse_transform(x.detach().numpy()), dtype=torch.float32)\n",
    "    recon_X_denormalized = torch.tensor(scaler_X.inverse_transform(recon_x.detach().numpy()), dtype=torch.float32)\n",
    "    \n",
    "    # Convert material_costs and embedded_carbon to torch tensors\n",
    "    material_costs_tensor = torch.tensor(material_costs, dtype=torch.float32)\n",
    "    embedded_carbon_tensor = torch.tensor(embedded_carbon, dtype=torch.float32)\n",
    "    \n",
    "    # Compute additional losses (material cost and carbon losses)\n",
    "    material_costs_loss = torch.sum(material_costs_tensor * recon_X_denormalized) - torch.sum(material_costs_tensor * X_denormalized)\n",
    "    embedded_carbon_loss = torch.sum(embedded_carbon_tensor * recon_X_denormalized) - torch.sum(embedded_carbon_tensor * X_denormalized)\n",
    "    \n",
    "    # Total loss\n",
    "    vae_loss = MSE + 0.001 * KLD \n",
    "    \n",
    "    # Return the square root of the total loss\n",
    "    return vae_loss+ c1 * material_costs_loss + c2 * embedded_carbon_loss\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "input_dim = X.shape[1]\n",
    "cond_dim = 1  # 'fc (MPa)' is the condition\n",
    "latent_dim = 2  # Size of latent space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layers_list = [\n",
    "    [16],\n",
    "    [32],\n",
    "    [64],\n",
    "    [128],\n",
    "    [256],\n",
    "    [32, 16],\n",
    "    [128, 64],\n",
    "    [256, 128],\n",
    "    [32, 16, 8],\n",
    "    [256, 128, 64],\n",
    "    [32, 16, 8, 4, 3]\n",
    "]\n",
    "\n",
    "decoder_layers_list = [\n",
    "    [16],\n",
    "    [32],\n",
    "    [64],\n",
    "    [128],\n",
    "    [256],\n",
    "    [16, 32],\n",
    "    [64, 128],\n",
    "    [128, 256],\n",
    "    [8, 16, 32],\n",
    "    [64, 128, 256],\n",
    "    [3, 4, 8, 16, 32]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_architectures = len(encoder_layers_list)\n",
    "for i in range(count_architectures):\n",
    "        # Example encoder architecture for this run\n",
    "    encoder_layers = encoder_layers_list[i]\n",
    "    decoder_layers = decoder_layers_list[i]\n",
    "    model = CVAE(input_dim, cond_dim, latent_dim,encoder_layers,decoder_layers)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Generate encoder name string\n",
    "    encoder_name = \"Encoder_\" + \"_\".join(str(x) for x in encoder_layers)\n",
    "\n",
    "    # Create file paths\n",
    "    loss_log_file = f\"output_vae_full_weight/output_file_vae_{encoder_name}_batch_32.csv\"\n",
    "    model_path = f\"output_vae_full_weight/cvae_model_{encoder_name}_batch_32.pth\"\n",
    "    optimizer_path = f\"output_vae_full_weight/cvae_optimizer_{encoder_name}_batch_32.pth\"\n",
    "\n",
    "    # Write CSV header\n",
    "    with open(loss_log_file, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Epoch\", \"Train Loss\", \"Test Loss\"])\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 5000\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            recon_x, mu, logvar = model(x_batch, y_batch)\n",
    "            # Calculate losses\n",
    "            loss = loss_function(recon_x, x_batch, mu, logvar)   \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Evaluation on test set\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                recon_x, mu, logvar = model(x_batch, y_batch)\n",
    "                loss = loss_function(recon_x, x_batch, mu, logvar)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        with open(loss_log_file, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch, train_loss/len(train_loader.dataset), test_loss/len(test_loader.dataset)])\n",
    "\n",
    "    # Save model and optimizer\n",
    "    torch.save(model, model_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
